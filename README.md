# Language Identification for Ghanaian Languages  

This project focuses on automatic Language Identification (LID) for four under-resourced Ghanaian languages: Akan, Ewe, Ga, and Dagbani.  
The goal is to build and evaluate models that can accurately classify a given text into its language category.  

---

## ğŸ“– Project Overview  
Language Identification has been well-studied for high-resource languages (English, French, Spanish, etc.), but local Ghanaian languages remain underrepresented.  
This project explores how traditional ML models (Logistic Regression, Naive Bayes, SVM) perform when trained on carefully collected and preprocessed corpora of Ghanaian languages.  

---

## ğŸ“‚ Repository Structure  


```
ghana_lid/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ preprocessing/            # Preprocessing scripts per language
â”‚   â”‚   â”œâ”€â”€ akan_preprocess.ipynb   # Akan preprocessing
â”‚   â”‚   â”œâ”€â”€ dagbani_preprocess.ipynb   # Dagbani preprocessing
â”‚   â”‚   â”œâ”€â”€ ewe_preprocess.ipynb    # Ewe preprocessing
â”‚   â”‚   â””â”€â”€ ga_preprocess.ipynb     # Ga preprocessing
â”‚   â”œâ”€â”€ processed/                # Cleaned & ready-to-use datasets
â”‚   â”‚   â”œâ”€â”€ dagbani_corpus.json
â”‚   â”‚   â”œâ”€â”€ ewe_corpus.json
â”‚   â”‚   â”œâ”€â”€ ga_corpus.json
â”‚   â”‚   â””â”€â”€ twi_corpus.json
â”‚   â””â”€â”€ README.md                 # Documentation + data source links
â”œâ”€â”€ main_notebook.ipynb           # Full pipeline (EDA â†’ Training â†’ Evaluation)
â”œâ”€â”€ requirements.txt              # Dependencies for reproducibility
â””â”€â”€ README.md                     # (this file)

```

---

## âš™ï¸ Installation & Setup  

Clone the repository:  
```bash
git clone https://github.com/hermes_4/ghana_lid.git
cd ghana_lid
```


Create and activate a virtual environment (recommended):

```bash
python -m venv venv
source venv/bin/activate   # for Linux/Mac
venv\Scripts\activate      # for Windows
```

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Getting Started  

After setting up the environment, you can start exploring the language identification pipeline:

1. **Open the main notebook:**
   ```bash
   jupyter notebook main_notebook.ipynb
   ```
   Or if using VS Code, simply open `main_notebook.ipynb` in the editor.

2. **Run the cells sequentially:**
   - Start from the top and execute each cell in order
   - The notebook includes data loading, exploratory analysis, feature engineering, model training, and evaluation
   - Each section is clearly documented with markdown explanations

3. **Expected workflow:**
   - **Data Loading:** Load preprocessed corpora for all four languages
   - **EDA:** Explore dataset statistics and sample texts
   - **Feature Engineering:** Convert text to TF-IDF vectors
   - **Model Training:** Train multiple classifiers (Naive Bayes, Logistic Regression, SVM)
   - **Evaluation:** Compare model performance with metrics and visualizations




